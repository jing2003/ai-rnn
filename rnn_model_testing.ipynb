{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6408f5f7b68c99bf",
   "metadata": {},
   "source": [
    "# **Recurrent Neural Network (RNN) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9998bb4ea9257fa",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7e3f126e01a8ee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T19:18:06.101004Z",
     "start_time": "2024-10-10T19:15:47.907929Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean and lemmatize a review\n",
    "def clean_review(text):\n",
    "    # start_time = time.time()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove digits and punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize the review\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords and apply lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    # print(f\"Time taken for cleaning a review: {time.time() - start_time:.2f} seconds\")\n",
    "    return words\n",
    "\n",
    "# Function to read and clean data for one review\n",
    "def process_file(file_path, label):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        review_text = file.read()\n",
    "        # Clean and lemmatize the review\n",
    "        cleaned_review = clean_review(review_text)\n",
    "    return cleaned_review, label\n",
    "\n",
    "# Function to read and clean data in parallel\n",
    "def load_data(data_dir):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    tasks = []\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for label_type in ['pos', 'neg']:\n",
    "            dir_name = os.path.join(data_dir, label_type)\n",
    "            label = 1 if label_type == 'pos' else 0\n",
    "            for file_name in os.listdir(dir_name):\n",
    "                file_path = os.path.join(dir_name, file_name)\n",
    "                # Submit tasks to process files in parallel\n",
    "                tasks.append(executor.submit(process_file, file_path, label))\n",
    "        \n",
    "        # Collect results as tasks complete\n",
    "        for task in as_completed(tasks):\n",
    "            review, label = task.result()\n",
    "            reviews.append(review)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return reviews, labels\n",
    "\n",
    "# Load train and test datasets\n",
    "# start_time = time.time()\n",
    "train_reviews, train_labels = load_data('train')\n",
    "# print(f\"Time taken to load train data: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# start_time = time.time()\n",
    "test_reviews, test_labels = load_data('test')\n",
    "# print(f\"Time taken to load test data: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# View an example of a cleaned review\n",
    "print(train_reviews[0])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yingy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yingy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yingy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['delightful', 'movie', 'overthetop', 'wife', 'daughter', 'found', 'irresistible', 'plot', 'crazy', 'ring', 'true', 'world', 'soap', 'opera', 'outrageous', 'improbability', 'impossibility', 'br', 'br', 'particularly', 'enjoyed', 'kevin', 'kline', 'sally', 'field', 'performance', 'dont', 'anyone', 'better', 'kline', 'playing', 'thickheaded', 'field', 'character', 'truly', 'desperate', 'need', 'attention', 'affirmation', 'almost', 'bipolar', 'swing', 'mood', 'played', 'nicely', 'background', 'field', 'famous', 'infamous', 'like', 'oscar', 'exclamation', 'people', 'take', 'large', 'grain', 'salt', 'rare', 'world', 'br', 'br', 'think', 'movie', 'didnt', 'find', 'impatient', 'whoopi', 'goldberg', 'characterization', 'thought', 'spot', 'every', 'note', 'struck', 'robert', 'downey', 'jr', 'teri', 'hatcher', 'cathy', 'moriarty', 'elizabeth', 'shue', 'also', 'firstrate', 'well', 'great', 'movie', 'youre', 'mood', 'go', 'along', 'ride', 'laugh']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "db3e3c395b9dc744",
   "metadata": {},
   "source": [
    "#### Vocabulary Extraction"
   ]
  },
  {
   "cell_type": "code",
   "id": "955690bfa1c90ce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T19:21:01.834863Z",
     "start_time": "2024-10-10T19:21:01.000873Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(reviews, max_vocab_size=10000):\n",
    "    # Flatten the list of reviews into a single list of words\n",
    "    all_words = [word for review in reviews for word in review]\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(all_words)\n",
    "    # Get the most common words up to max_vocab_size\n",
    "    most_common_words = word_counts.most_common(max_vocab_size)\n",
    "    # Create a vocabulary dictionary, starting with special tokens\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}  # Padding token and unknown token\n",
    "    for idx, (word, _) in enumerate(most_common_words, start=2):\n",
    "        vocab[word] = idx\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary from train reviews\n",
    "vocab = build_vocab(train_reviews)\n",
    "\n",
    "# View the size of the vocabulary and an example of the vocab\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"First 10 words in the vocabulary: {list(vocab.items())[:10]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10002\n",
      "First 10 words in the vocabulary: [('<PAD>', 0), ('<UNK>', 1), ('br', 2), ('movie', 3), ('film', 4), ('one', 5), ('like', 6), ('time', 7), ('good', 8), ('character', 9)]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "80d1ef977f7084c8",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "7857f25981b8be6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T19:21:11.395777Z",
     "start_time": "2024-10-10T19:21:07.312395Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Function to encode reviews as sequences of indices\n",
    "def encode_review(review, vocab, max_len):\n",
    "    encoded = [vocab.get(word, vocab['<UNK>']) for word in review]\n",
    "    # Pad or truncate the encoded review\n",
    "    if len(encoded) > max_len:\n",
    "        return encoded[:max_len]\n",
    "    else:\n",
    "        return encoded + [vocab['<PAD>']] * (max_len - len(encoded))\n",
    "\n",
    "# Encode and pad all reviews\n",
    "max_len = 200  # Maximum sequence length\n",
    "train_encoded = [encode_review(review, vocab, max_len) for review in train_reviews]\n",
    "test_encoded = [encode_review(review, vocab, max_len) for review in test_reviews]\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "train_encoded = torch.tensor(train_encoded)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_encoded = torch.tensor(test_encoded)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create a custom dataset class\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.labels[idx]\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = ReviewDataset(train_encoded, train_labels)\n",
    "test_dataset = ReviewDataset(test_encoded, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c7b64009c6d999d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T19:21:13.739858Z",
     "start_time": "2024-10-10T19:21:13.700401Z"
    }
   },
   "source": [
    "# RNN model\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<PAD>'])\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "        # RNN layer\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # Fully connected layer (we use the hidden state of the last RNN cell)\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 1  # Binary classification (positive or negative)\n",
    "\n",
    "# Instantiate the model\n",
    "model = SentimentRNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam Optimizer"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "87f23d47d047e2bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T19:21:24.649805Z",
     "start_time": "2024-10-10T19:21:22.637829Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to train model and print training convergence plot\n",
    "def training(model):\n",
    "    num_epochs = 10\n",
    "    loss_values = []\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for reviews, labels in train_loader:\n",
    "            output = model(reviews)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get the index of the max log-probability\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "            # Append predictions and labels to lists\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate average loss\n",
    "        average_loss = running_loss / len(train_loader)\n",
    "        loss_values.append(average_loss)\n",
    "        \n",
    "        # Convert the lists to numpy arrays\n",
    "        all_preds = torch.tensor(all_preds)\n",
    "        all_labels = torch.tensor(all_labels)\n",
    "        \n",
    "        # Calculate Accuracy, Precision, Recall, F1 Score\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, Loss: {average_loss:.4f}\")\n",
    "    \n",
    "    # Plotting the convergence plot\n",
    "    plt.plot(loss_values, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Convergence Plot (Loss Over Epochs)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ca8817eb8d445a7",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-10T19:21:28.215645Z"
    }
   },
   "source": [
    "training(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba29bca2810d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on test data\n",
    "def evaluating(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for reviews, labels in data_loader:\n",
    "            output = model(reviews)\n",
    "            predictions = torch.round(torch.sigmoid(output.squeeze())).cpu().numpy()\n",
    "            all_preds.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert the lists to numpy arrays\n",
    "    all_preds = torch.tensor(all_preds)\n",
    "    all_labels = torch.tensor(all_labels)\n",
    "\n",
    "    # Calculate Accuracy, Precision, Recall, F1 Score\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8582245e3cb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluating(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571b3a481030701",
   "metadata": {},
   "source": [
    "### Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bccde8748f4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model 2\n",
    "class SimpleSentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SimpleSentimentRNN, self).__init__()\n",
    "        # Simpler embedding layer with reduced dimensions\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<PAD>'])\n",
    "        # Simpler RNN with fewer hidden units\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # Word embeddings\n",
    "        output, hidden = self.rnn(embedded)  # RNN layer\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "# Instantiate the model with reduced embedding and hidden dimensions\n",
    "model_2 = SimpleSentimentRNN(vocab_size=1000, embedding_dim=10, hidden_dim=32, output_dim=1)\n",
    "\n",
    "training(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f2d2e326bad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluating(model_2, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
